{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some cell\n",
    "\n",
    "Some content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional network\n",
    "\n",
    "### Architecture\n",
    "\n",
    "#### First conv layer\n",
    "\n",
    "**Q:** Why do first conv layers in NNs usually have kernel size larger than 3x3?\n",
    "\n",
    ">The **first conv layer often has a kernel size larger than 3x3** (say 5x5 or 7x7) because a 3 x 3 kernel does not effectively compress the data.\n",
    "E.g. on an RGB image, a first layer with 32 3x3 kernels would convert a volume of size 3 x 3 x 3 = 27 (kernel x rgb depth) into a volume of size 1 x 32 = 32.\n",
    ">\n",
    ">Compare this to a 7x7 kernel, which reduces a volume of size 147 to 32.\n",
    "Therefore, when designing an architecure, one wants to think about how many number go in and how many come out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telemetry\n",
    "\n",
    "Using **hooks** JH monitors the mean and std of each layer's activations in a list to be able to track how they evolve over time.\n",
    "\n",
    "**Q:** What can you detect by monitoring model activations's means, stds and histogram?\n",
    "\n",
    ">By observing the evolution of model activations, one might be able to detect exploding/ vanishing gradients.\n",
    ">\n",
    "> Things to look out for are\n",
    ">\n",
    ">    * Means should stay around zero and stds around one throughout each layer.\n",
    ">    * Values should not diverge or osciallte too strongly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hooks\n",
    "\n",
    "A pytorch nn.Module (layer or model) can register a hook by `m.register_forward_hook(func: Callable[[nn.Module, Tensor, Tensor], None])` where callable expects the nn.Module and the input and output activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized ReLU\n",
    "\n",
    "**Q:** What are problems with a ReLU layer's output activations?\n",
    "\n",
    ">Output activations generally do not have mean=0 and std=1 because of the ReLU throwing away all negative parts of the inputs. This shifts the mean to values larger than 0.\n",
    "\n",
    "A **generalized ReLU** can be set up to modify the standard ReLU function by adding **leakage** so that it can go negative, **clamping at a max value** and **subtracting an offset** in order to shift the output's mean back to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
